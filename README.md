# Ollama Assistant for VS Code

VS Code extension that integrates local Ollama models for Copilot-like development assistance, with total control over your data.

## ‚ú® Features

- **Intelligent completion** : Real-time code suggestions
- **Code generation** : Via dedicated command (`Ctrl+Shift+P > Generate Code`)
- **Interactive chat** : Integrated conversational interface
- **Multi-models** : Support for all Ollama models (CodeLlama, Mistral, etc.). )
- **Local execution** : No data sent to the cloud

## üöÄ Installation

1. Install [Ollama](https://ollama.ai) and start the service:
   
 curl -fsSL https://ollama.ai/install. sh | sh
 ollama serve
   
Download a template (e.g. CodeLlama):

 ollama pull codellama

Install the extension in VS Code:

Via the marketplace (recommended)
Search for "Ollama Assistant"

Manually:
        
 npm install
 npm run compile
 vsce package
 code --install-extension ollama-assistant-0. 0.1.vsix

‚öôÔ∏è Configuration

Access settings (Ctrl+,) and search for "Ollama":

 Endpoint: http://localhost:11434 (default)

 Default Model: codellama (modifiable)

üéØ Usage
Auto-complete

Start typing to see suggestions generated by the template.

Main commands

 Command Shortcut Description
 Generate Code Ctrl+Alt+G Generates code from a prompt
 Open Chat Ctrl+Alt+C Opens the chat panel
 Status bar

Displays the active template

 Click to change template

üõ† Development

Contributions welcome! Here's how to set up the environment:

  git clone https://github.com/Phil974/ollama-assistant.git
  cd ollama-assistant
  npm install
  npm run watch # Real-time compilation

üìú License

 MIT - Read the license at https://mit-license.org/

 Note: This extension requires Ollama v0.1.0+. To update: 

 ollama --version # to know the ollama version


=======
# ollama-assistant

